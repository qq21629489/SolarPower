{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "# 0~0.7 training\n",
    "# 0.7~0.9 testing\n",
    "# 0.9~1.0 validation\n",
    "TRAIN_THRESH = 0.7\n",
    "TEST_THRESH = 0.9\n",
    "REF_SIZE = 24 * 7 # for one week\n",
    "PREDICT_SIZE = 1 # for one day\n",
    "BATCH_SIZE = 128\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file ./training_data\\target0.csv (1/50)\n",
      "loading file ./training_data\\target1.csv (2/50)\n",
      "loading file ./training_data\\target10.csv (3/50)\n",
      "loading file ./training_data\\target11.csv (4/50)\n",
      "loading file ./training_data\\target12.csv (5/50)\n",
      "loading file ./training_data\\target13.csv (6/50)\n",
      "loading file ./training_data\\target14.csv (7/50)\n",
      "loading file ./training_data\\target15.csv (8/50)\n",
      "loading file ./training_data\\target16.csv (9/50)\n",
      "loading file ./training_data\\target17.csv (10/50)\n",
      "loading file ./training_data\\target18.csv (11/50)\n",
      "loading file ./training_data\\target19.csv (12/50)\n",
      "loading file ./training_data\\target2.csv (13/50)\n",
      "loading file ./training_data\\target20.csv (14/50)\n",
      "loading file ./training_data\\target21.csv (15/50)\n",
      "loading file ./training_data\\target22.csv (16/50)\n",
      "loading file ./training_data\\target23.csv (17/50)\n",
      "loading file ./training_data\\target24.csv (18/50)\n",
      "loading file ./training_data\\target25.csv (19/50)\n",
      "loading file ./training_data\\target26.csv (20/50)\n",
      "loading file ./training_data\\target27.csv (21/50)\n",
      "loading file ./training_data\\target28.csv (22/50)\n",
      "loading file ./training_data\\target29.csv (23/50)\n",
      "loading file ./training_data\\target3.csv (24/50)\n",
      "loading file ./training_data\\target30.csv (25/50)\n",
      "loading file ./training_data\\target31.csv (26/50)\n",
      "loading file ./training_data\\target32.csv (27/50)\n",
      "loading file ./training_data\\target33.csv (28/50)\n",
      "loading file ./training_data\\target34.csv (29/50)\n",
      "loading file ./training_data\\target35.csv (30/50)\n",
      "loading file ./training_data\\target36.csv (31/50)\n",
      "loading file ./training_data\\target37.csv (32/50)\n",
      "loading file ./training_data\\target38.csv (33/50)\n",
      "loading file ./training_data\\target39.csv (34/50)\n",
      "loading file ./training_data\\target4.csv (35/50)\n",
      "loading file ./training_data\\target40.csv (36/50)\n",
      "loading file ./training_data\\target41.csv (37/50)\n",
      "loading file ./training_data\\target42.csv (38/50)\n",
      "loading file ./training_data\\target43.csv (39/50)\n",
      "loading file ./training_data\\target44.csv (40/50)\n",
      "loading file ./training_data\\target45.csv (41/50)\n",
      "loading file ./training_data\\target46.csv (42/50)\n",
      "loading file ./training_data\\target47.csv (43/50)\n",
      "loading file ./training_data\\target48.csv (44/50)\n",
      "loading file ./training_data\\target49.csv (45/50)\n",
      "loading file ./training_data\\target5.csv (46/50)\n",
      "loading file ./training_data\\target6.csv (47/50)\n",
      "loading file ./training_data\\target7.csv (48/50)\n",
      "loading file ./training_data\\target8.csv (49/50)\n",
      "loading file ./training_data\\target9.csv (50/50)\n",
      "      generation  consumption  month  day  hour\n",
      "0           0.00         0.05      1    1     0\n",
      "1           0.00         1.52      1    1     1\n",
      "2           0.01         1.09      1    1     2\n",
      "3           0.00         0.95      1    1     3\n",
      "4           0.00         0.75      1    1     4\n",
      "...          ...          ...    ...  ...   ...\n",
      "5827        0.03         3.42      8   31    19\n",
      "5828        0.00         3.55      8   31    20\n",
      "5829        0.00         3.07      8   31    21\n",
      "5830        0.00         2.34      8   31    22\n",
      "5831        0.00         2.04      8   31    23\n",
      "\n",
      "[5832 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def loadData():\n",
    "    paths = glob.glob(r'./training_data/*.csv')\n",
    "    datas = []\n",
    "    max_dict = {'generation':-1.0, 'consumption':-1.0}\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        print('loading file {} ({}/{})'.format(paths[i], i + 1, len(paths)))\n",
    "        df = pd.read_csv(paths[i])\n",
    "        \n",
    "        g_max, c_max = df['generation'].max(), df['consumption'].max()\n",
    "        max_dict['generation'] = g_max if g_max > max_dict['generation'] else max_dict['generation']\n",
    "        max_dict['consumption'] = c_max if c_max > max_dict['consumption'] else max_dict['consumption']\n",
    "        \n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['month'] = df['time'].dt.month\n",
    "        df['day'] = df['time'].dt.day\n",
    "        df['hour'] = df['time'].dt.hour\n",
    "        df = df.drop(['time'], axis=1)\n",
    "        datas.append(df)\n",
    "    # print(datas)\n",
    "    # print(max_dict)\n",
    "    return datas, max_dict\n",
    "\n",
    "datas, max_dict = loadData()\n",
    "print(datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setXY(datas, ref_size, predict_size, type_='generation'):\n",
    "    x, y = [], []\n",
    "    \n",
    "    for index, data in enumerate(datas):\n",
    "        for j in range(len(data) - ref_size -  predict_size):\n",
    "            # if type_ == 'generation':\n",
    "            #     data.drop('consumption', axis=1)\n",
    "            # elif type_ == 'consumption':\n",
    "            #     data.drop('generation', axis=1)\n",
    "            \n",
    "            x.append(np.array(data.iloc[j: j + ref_size]))\n",
    "            y.append(np.array(data.iloc[j + ref_size: j + ref_size + predict_size][type_]))\n",
    "        print('{} done'.format(index))\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    np.random.seed(int(time.time()))\n",
    "    randomList = np.arange(x.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return x[randomList], y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLSTM(input_shape, output_shape):\n",
    "    # model = Sequential()\n",
    "    # model.add(LSTM(units=900, return_sequences = True, kernel_initializer = 'glorot_uniform', input_shape = (input_shape[1], input_shape[2])))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(LSTM(units = 900, kernel_initializer = 'glorot_uniform', return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(units = 300, kernel_initializer = 'glorot_uniform', return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(units = 300, kernel_initializer = 'glorot_uniform', return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(units = 300, kernel_initializer = 'glorot_uniform', return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(units = 300, kernel_initializer = 'glorot_uniform'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(units = output_shape))\n",
    "    # model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mse'])\n",
    "    # model.summary()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(input_shape[1], input_shape[2]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a second LSTM layer and some Dropout regularization\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a second LSTM layer and some Dropout regularization\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a fourth LSTM layer and some Dropout regularization\n",
    "    model.add(TimeDistributed(Dense(units = 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mse'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "# lstm_model_c = buildLSTM(train_x.shape, PREDICT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "(283150, 168, 5)\n",
      "(283150, 1)\n",
      "(198205, 168, 5) (198205, 1)\n",
      "(56630, 168, 5) (56630, 1)\n",
      "(28315, 168, 5) (28315, 1)\n"
     ]
    }
   ],
   "source": [
    "x, y = setXY(datas, REF_SIZE, PREDICT_SIZE)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# split train test\n",
    "# 0~0.7 training\n",
    "# 0.7~0.9 testing\n",
    "# 0.9~1.0 validation\n",
    "train_x, train_y = shuffle(x[:int(x.shape[0] * TRAIN_THRESH)], y[:int(y.shape[0] * TRAIN_THRESH)])\n",
    "test_x, test_y   = shuffle(x[int(x.shape[0] * TRAIN_THRESH): int(y.shape[0] * TEST_THRESH)], y[int(y.shape[0] * TRAIN_THRESH): int(y.shape[0] * TEST_THRESH)])\n",
    "val_x, val_y     = shuffle(x[int(x.shape[0] * TEST_THRESH):], y[int(y.shape[0] * TEST_THRESH):])\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 168, 64)           17920     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 168, 64)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 168, 64)           33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 168, 64)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 168, 64)           33024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 168, 64)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 168, 1)           65        \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 168)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               86528     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,074\n",
      "Trainable params: 171,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1549/1549 [==============================] - 108s 61ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 2/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.0586 - val_mse: 0.0586\n",
      "Epoch 3/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 4/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0551 - mse: 0.0551 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 5/100\n",
      "1549/1549 [==============================] - 92s 60ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 6/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 7/100\n",
      "1549/1549 [==============================] - 97s 62ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 8/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 9/100\n",
      "1549/1549 [==============================] - 91s 58ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 10/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 11/100\n",
      "1549/1549 [==============================] - 95s 61ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 12/100\n",
      "1549/1549 [==============================] - 90s 58ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 13/100\n",
      "1549/1549 [==============================] - 92s 60ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 14/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 15/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 16/100\n",
      "1549/1549 [==============================] - 92s 60ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 17/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 18/100\n",
      "1549/1549 [==============================] - 92s 60ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 19/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 20/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 21/100\n",
      "1549/1549 [==============================] - 90s 58ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 22/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 23/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 24/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 25/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 26/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 27/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 28/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 29/100\n",
      "1549/1549 [==============================] - 88s 57ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 30/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 31/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 32/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 33/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 34/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 35/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 36/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 37/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 38/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 39/100\n",
      "1549/1549 [==============================] - 92s 60ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 40/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 41/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 42/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 43/100\n",
      "1549/1549 [==============================] - 90s 58ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 44/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 45/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 46/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 47/100\n",
      "1549/1549 [==============================] - 96s 62ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 48/100\n",
      "1549/1549 [==============================] - 105s 68ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 49/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 50/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 51/100\n",
      "1549/1549 [==============================] - 98s 63ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 52/100\n",
      "1549/1549 [==============================] - 97s 62ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 53/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 54/100\n",
      "1549/1549 [==============================] - 103s 67ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 55/100\n",
      "1549/1549 [==============================] - 101s 65ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 56/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 57/100\n",
      "1549/1549 [==============================] - 95s 61ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 00057: early stopping\n"
     ]
    }
   ],
   "source": [
    "lstm_modelG = buildLSTM(train_x.shape, PREDICT_SIZE)\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=PATIENCE, verbose=1, mode='min')\n",
    "historyG = lstm_modelG.fit(train_x, train_y, verbose=1, callbacks=[early_stopping], validation_data=(val_x, val_y), batch_size=BATCH_SIZE, epochs=EPOCH)\n",
    "\n",
    "lstm_modelG.save('modeG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "(283150, 168, 5)\n",
      "(283150, 1)\n",
      "(198205, 168, 5) (198205, 1)\n",
      "(56630, 168, 5) (56630, 1)\n",
      "(28315, 168, 5) (28315, 1)\n"
     ]
    }
   ],
   "source": [
    "x, y = setXY(datas, REF_SIZE, PREDICT_SIZE, type_='consumption')\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# split train test\n",
    "# 0~0.7 training\n",
    "# 0.7~0.9 testing\n",
    "# 0.9~1.0 validation\n",
    "train_x, train_y = shuffle(x[:int(x.shape[0] * TRAIN_THRESH)], y[:int(y.shape[0] * TRAIN_THRESH)])\n",
    "test_x, test_y   = shuffle(x[int(x.shape[0] * TRAIN_THRESH): int(y.shape[0] * TEST_THRESH)], y[int(y.shape[0] * TRAIN_THRESH): int(y.shape[0] * TEST_THRESH)])\n",
    "val_x, val_y     = shuffle(x[int(x.shape[0] * TEST_THRESH):], y[int(y.shape[0] * TEST_THRESH):])\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 168, 64)           17920     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 168, 64)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 168, 64)           33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 168, 64)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 168, 64)           33024     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 168, 64)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 168, 1)           65        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 168)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               86528     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,074\n",
      "Trainable params: 171,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1549/1549 [==============================] - 202s 122ms/step - loss: 0.5796 - mse: 0.5796 - val_loss: 0.4430 - val_mse: 0.4430\n",
      "Epoch 2/100\n",
      "1549/1549 [==============================] - 144s 93ms/step - loss: 0.3676 - mse: 0.3676 - val_loss: 0.4183 - val_mse: 0.4183\n",
      "Epoch 3/100\n",
      "1549/1549 [==============================] - 105s 68ms/step - loss: 0.3355 - mse: 0.3355 - val_loss: 0.3740 - val_mse: 0.3740\n",
      "Epoch 4/100\n",
      "1549/1549 [==============================] - 114s 74ms/step - loss: 0.3031 - mse: 0.3031 - val_loss: 0.3336 - val_mse: 0.3336\n",
      "Epoch 5/100\n",
      "1549/1549 [==============================] - 214s 138ms/step - loss: 0.2693 - mse: 0.2693 - val_loss: 0.2917 - val_mse: 0.2917\n",
      "Epoch 6/100\n",
      "1549/1549 [==============================] - 123s 79ms/step - loss: 0.2395 - mse: 0.2395 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 7/100\n",
      "1549/1549 [==============================] - 99s 64ms/step - loss: 0.2173 - mse: 0.2173 - val_loss: 0.2784 - val_mse: 0.2784\n",
      "Epoch 8/100\n",
      "1549/1549 [==============================] - 103s 67ms/step - loss: 0.2014 - mse: 0.2014 - val_loss: 0.2300 - val_mse: 0.2300\n",
      "Epoch 9/100\n",
      "1549/1549 [==============================] - 103s 66ms/step - loss: 0.1872 - mse: 0.1872 - val_loss: 0.2221 - val_mse: 0.2221\n",
      "Epoch 10/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.2080 - val_mse: 0.2080\n",
      "Epoch 11/100\n",
      "1549/1549 [==============================] - 97s 63ms/step - loss: 0.1710 - mse: 0.1710 - val_loss: 0.1988 - val_mse: 0.1988\n",
      "Epoch 12/100\n",
      "1549/1549 [==============================] - 95s 62ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 0.2011 - val_mse: 0.2011\n",
      "Epoch 13/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.1592 - mse: 0.1592 - val_loss: 0.2008 - val_mse: 0.2008\n",
      "Epoch 14/100\n",
      "1549/1549 [==============================] - 118s 76ms/step - loss: 0.1537 - mse: 0.1537 - val_loss: 0.1822 - val_mse: 0.1822\n",
      "Epoch 15/100\n",
      "1549/1549 [==============================] - 104s 67ms/step - loss: 0.1482 - mse: 0.1482 - val_loss: 0.1838 - val_mse: 0.1838\n",
      "Epoch 16/100\n",
      "1549/1549 [==============================] - 132s 86ms/step - loss: 0.1444 - mse: 0.1444 - val_loss: 0.1699 - val_mse: 0.1699\n",
      "Epoch 17/100\n",
      "1549/1549 [==============================] - 116s 75ms/step - loss: 0.1404 - mse: 0.1404 - val_loss: 0.1665 - val_mse: 0.1665\n",
      "Epoch 18/100\n",
      "1549/1549 [==============================] - 91s 59ms/step - loss: 0.1370 - mse: 0.1370 - val_loss: 0.1658 - val_mse: 0.1658\n",
      "Epoch 19/100\n",
      "1549/1549 [==============================] - 97s 63ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1831 - val_mse: 0.1831\n",
      "Epoch 20/100\n",
      "1549/1549 [==============================] - 107s 69ms/step - loss: 0.1314 - mse: 0.1314 - val_loss: 0.1597 - val_mse: 0.1597\n",
      "Epoch 21/100\n",
      "1549/1549 [==============================] - 97s 62ms/step - loss: 0.1298 - mse: 0.1298 - val_loss: 0.1552 - val_mse: 0.1552\n",
      "Epoch 22/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.1277 - mse: 0.1277 - val_loss: 0.1525 - val_mse: 0.1525\n",
      "Epoch 23/100\n",
      "1549/1549 [==============================] - 98s 63ms/step - loss: 0.1249 - mse: 0.1249 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 24/100\n",
      "1549/1549 [==============================] - 146s 94ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1492 - val_mse: 0.1492\n",
      "Epoch 25/100\n",
      "1549/1549 [==============================] - 97s 63ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.1444 - val_mse: 0.1444\n",
      "Epoch 26/100\n",
      "1549/1549 [==============================] - 156s 101ms/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 27/100\n",
      "1549/1549 [==============================] - 97s 62ms/step - loss: 0.1179 - mse: 0.1179 - val_loss: 0.1442 - val_mse: 0.1442\n",
      "Epoch 28/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.1153 - mse: 0.1153 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 29/100\n",
      "1549/1549 [==============================] - 98s 64ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 0.1382 - val_mse: 0.1382\n",
      "Epoch 30/100\n",
      "1549/1549 [==============================] - 151s 97ms/step - loss: 0.1120 - mse: 0.1120 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 31/100\n",
      "1549/1549 [==============================] - 202s 131ms/step - loss: 0.1125 - mse: 0.1125 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 32/100\n",
      "1549/1549 [==============================] - 107s 69ms/step - loss: 0.1097 - mse: 0.1097 - val_loss: 0.1350 - val_mse: 0.1350\n",
      "Epoch 33/100\n",
      "1549/1549 [==============================] - 104s 67ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 34/100\n",
      "1549/1549 [==============================] - 123s 79ms/step - loss: 0.1072 - mse: 0.1072 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 35/100\n",
      "1549/1549 [==============================] - 99s 64ms/step - loss: 0.1072 - mse: 0.1072 - val_loss: 0.1533 - val_mse: 0.1533\n",
      "Epoch 36/100\n",
      "1549/1549 [==============================] - 112s 72ms/step - loss: 0.1051 - mse: 0.1051 - val_loss: 0.1300 - val_mse: 0.1300\n",
      "Epoch 37/100\n",
      "1549/1549 [==============================] - 95s 61ms/step - loss: 0.1049 - mse: 0.1049 - val_loss: 0.1386 - val_mse: 0.1386\n",
      "Epoch 38/100\n",
      "1549/1549 [==============================] - 100s 64ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.1476 - val_mse: 0.1476\n",
      "Epoch 39/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.1030 - mse: 0.1030 - val_loss: 0.1322 - val_mse: 0.1322\n",
      "Epoch 40/100\n",
      "1549/1549 [==============================] - 101s 65ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1375 - val_mse: 0.1375\n",
      "Epoch 41/100\n",
      "1549/1549 [==============================] - 97s 63ms/step - loss: 0.1005 - mse: 0.1005 - val_loss: 0.1298 - val_mse: 0.1298\n",
      "Epoch 42/100\n",
      "1549/1549 [==============================] - 99s 64ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.1306 - val_mse: 0.1306\n",
      "Epoch 43/100\n",
      "1549/1549 [==============================] - 101s 65ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 44/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.1320 - val_mse: 0.1320\n",
      "Epoch 45/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.1298 - val_mse: 0.1298\n",
      "Epoch 46/100\n",
      "1549/1549 [==============================] - 105s 68ms/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.1261 - val_mse: 0.1261\n",
      "Epoch 47/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.0948 - mse: 0.0948 - val_loss: 0.1321 - val_mse: 0.1321\n",
      "Epoch 48/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.0957 - mse: 0.0957 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 49/100\n",
      "1549/1549 [==============================] - 98s 63ms/step - loss: 0.0943 - mse: 0.0943 - val_loss: 0.1256 - val_mse: 0.1256\n",
      "Epoch 50/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.1233 - val_mse: 0.1233\n",
      "Epoch 51/100\n",
      "1549/1549 [==============================] - 108s 70ms/step - loss: 0.0934 - mse: 0.0934 - val_loss: 0.1280 - val_mse: 0.1280\n",
      "Epoch 52/100\n",
      "1549/1549 [==============================] - 109s 70ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 53/100\n",
      "1549/1549 [==============================] - 121s 78ms/step - loss: 0.0927 - mse: 0.0927 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 54/100\n",
      "1549/1549 [==============================] - 107s 69ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 55/100\n",
      "1549/1549 [==============================] - 104s 67ms/step - loss: 0.0911 - mse: 0.0911 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 56/100\n",
      "1549/1549 [==============================] - 92s 59ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.1274 - val_mse: 0.1274\n",
      "Epoch 57/100\n",
      "1549/1549 [==============================] - 97s 63ms/step - loss: 0.0894 - mse: 0.0894 - val_loss: 0.1250 - val_mse: 0.1250\n",
      "Epoch 58/100\n",
      "1549/1549 [==============================] - 106s 68ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 59/100\n",
      "1549/1549 [==============================] - 118s 76ms/step - loss: 0.0886 - mse: 0.0886 - val_loss: 0.1256 - val_mse: 0.1256\n",
      "Epoch 60/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.0885 - mse: 0.0885 - val_loss: 0.1201 - val_mse: 0.1201\n",
      "Epoch 61/100\n",
      "1549/1549 [==============================] - 93s 60ms/step - loss: 0.0881 - mse: 0.0881 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 62/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1245 - val_mse: 0.1245\n",
      "Epoch 63/100\n",
      "1549/1549 [==============================] - 96s 62ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.1245 - val_mse: 0.1245\n",
      "Epoch 64/100\n",
      "1549/1549 [==============================] - 121s 78ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.1295 - val_mse: 0.1295\n",
      "Epoch 65/100\n",
      "1549/1549 [==============================] - 114s 74ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 66/100\n",
      "1549/1549 [==============================] - 106s 68ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.1219 - val_mse: 0.1219\n",
      "Epoch 67/100\n",
      "1549/1549 [==============================] - 107s 69ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1175 - val_mse: 0.1175\n",
      "Epoch 68/100\n",
      "1549/1549 [==============================] - 120s 77ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.1237 - val_mse: 0.1237\n",
      "Epoch 69/100\n",
      "1549/1549 [==============================] - 109s 70ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1139 - val_mse: 0.1139\n",
      "Epoch 70/100\n",
      "1549/1549 [==============================] - 104s 67ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 71/100\n",
      "1549/1549 [==============================] - 128s 82ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.1181 - val_mse: 0.1181\n",
      "Epoch 72/100\n",
      "1549/1549 [==============================] - 107s 69ms/step - loss: 0.0828 - mse: 0.0828 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 73/100\n",
      "1549/1549 [==============================] - 98s 63ms/step - loss: 0.0821 - mse: 0.0821 - val_loss: 0.1169 - val_mse: 0.1169\n",
      "Epoch 74/100\n",
      "1549/1549 [==============================] - 96s 62ms/step - loss: 0.0812 - mse: 0.0812 - val_loss: 0.1146 - val_mse: 0.1146\n",
      "Epoch 75/100\n",
      "1549/1549 [==============================] - 126s 81ms/step - loss: 0.0821 - mse: 0.0821 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 76/100\n",
      "1549/1549 [==============================] - 116s 74ms/step - loss: 0.0809 - mse: 0.0809 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 77/100\n",
      "1549/1549 [==============================] - 94s 61ms/step - loss: 0.0811 - mse: 0.0811 - val_loss: 0.1200 - val_mse: 0.1200\n",
      "Epoch 78/100\n",
      "1549/1549 [==============================] - 110s 71ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 79/100\n",
      "1549/1549 [==============================] - 146s 93ms/step - loss: 0.0809 - mse: 0.0809 - val_loss: 0.1131 - val_mse: 0.1131\n",
      "Epoch 80/100\n",
      "1549/1549 [==============================] - 161s 104ms/step - loss: 0.0811 - mse: 0.0811 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 81/100\n",
      "1549/1549 [==============================] - 109s 70ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.1200 - val_mse: 0.1200\n",
      "Epoch 82/100\n",
      "1549/1549 [==============================] - 106s 69ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.1177 - val_mse: 0.1177\n",
      "Epoch 83/100\n",
      "1549/1549 [==============================] - 121s 78ms/step - loss: 0.0789 - mse: 0.0789 - val_loss: 0.1178 - val_mse: 0.1178\n",
      "Epoch 84/100\n",
      "1549/1549 [==============================] - 96s 62ms/step - loss: 0.0783 - mse: 0.0783 - val_loss: 0.1190 - val_mse: 0.1190\n",
      "Epoch 85/100\n",
      "1549/1549 [==============================] - 112s 72ms/step - loss: 0.0785 - mse: 0.0785 - val_loss: 0.1147 - val_mse: 0.1147\n",
      "Epoch 86/100\n",
      "1549/1549 [==============================] - 102s 66ms/step - loss: 0.0776 - mse: 0.0776 - val_loss: 0.1185 - val_mse: 0.1185\n",
      "Epoch 87/100\n",
      "1549/1549 [==============================] - 108s 70ms/step - loss: 0.0778 - mse: 0.0778 - val_loss: 0.1130 - val_mse: 0.1130\n",
      "Epoch 88/100\n",
      "1549/1549 [==============================] - 100s 65ms/step - loss: 0.0766 - mse: 0.0766 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 89/100\n",
      "1549/1549 [==============================] - 106s 68ms/step - loss: 0.0761 - mse: 0.0761 - val_loss: 0.1143 - val_mse: 0.1143\n",
      "Epoch 90/100\n",
      "1549/1549 [==============================] - 169s 109ms/step - loss: 0.0769 - mse: 0.0769 - val_loss: 0.1120 - val_mse: 0.1120\n",
      "Epoch 91/100\n",
      "1549/1549 [==============================] - 118s 76ms/step - loss: 0.0762 - mse: 0.0762 - val_loss: 0.1154 - val_mse: 0.1154\n",
      "Epoch 92/100\n",
      "1549/1549 [==============================] - 108s 70ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.1151 - val_mse: 0.1151\n",
      "Epoch 93/100\n",
      "1549/1549 [==============================] - 129s 84ms/step - loss: 0.0760 - mse: 0.0760 - val_loss: 0.1132 - val_mse: 0.1132\n",
      "Epoch 94/100\n",
      "1549/1549 [==============================] - 101s 65ms/step - loss: 0.0760 - mse: 0.0760 - val_loss: 0.1169 - val_mse: 0.1169\n",
      "Epoch 95/100\n",
      "1549/1549 [==============================] - 127s 82ms/step - loss: 0.0751 - mse: 0.0751 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 96/100\n",
      "1549/1549 [==============================] - 99s 64ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.1123 - val_mse: 0.1123\n",
      "Epoch 97/100\n",
      "1549/1549 [==============================] - 134s 87ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.1152 - val_mse: 0.1152\n",
      "Epoch 98/100\n",
      "1549/1549 [==============================] - 99s 64ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 99/100\n",
      "1549/1549 [==============================] - 142s 91ms/step - loss: 0.0751 - mse: 0.0751 - val_loss: 0.1196 - val_mse: 0.1196\n",
      "Epoch 100/100\n",
      "1549/1549 [==============================] - 126s 81ms/step - loss: 0.0742 - mse: 0.0742 - val_loss: 0.1130 - val_mse: 0.1130\n",
      "Epoch 00100: early stopping\n"
     ]
    }
   ],
   "source": [
    "lstm_modelC = buildLSTM(train_x.shape, PREDICT_SIZE)\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=PATIENCE, verbose=1, mode='min')\n",
    "historyC = lstm_modelC.fit(train_x, train_y, verbose=1, callbacks=[early_stopping], validation_data=(val_x, val_y), batch_size=BATCH_SIZE, epochs=EPOCH)\n",
    "\n",
    "lstm_modelC.save('modeC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPredictData():\n",
    "    dfG = pd.read_csv('sample_data\\generation.csv')\n",
    "    dfC = pd.read_csv('sample_data\\consumption.csv')\n",
    "    \n",
    "    df = pd.merge(dfG, dfC, on='time')\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['month'] = df['time'].dt.month\n",
    "    df['day'] = df['time'].dt.dayofweek\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df = df.drop(['time'], axis=1)\n",
    "    # print(datas)\n",
    "    # print(max_dict)\n",
    "    return df\n",
    "\n",
    "data_to_predict = loadPredictData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-05 00:00:00 done\n",
      "2018-08-05 01:00:00 done\n",
      "2018-08-05 02:00:00 done\n",
      "2018-08-05 03:00:00 done\n",
      "2018-08-05 04:00:00 done\n",
      "2018-08-05 05:00:00 done\n",
      "2018-08-05 06:00:00 done\n",
      "2018-08-05 07:00:00 done\n",
      "2018-08-05 08:00:00 done\n",
      "2018-08-05 09:00:00 done\n",
      "2018-08-05 10:00:00 done\n",
      "2018-08-05 11:00:00 done\n",
      "2018-08-05 12:00:00 done\n",
      "2018-08-05 13:00:00 done\n",
      "2018-08-05 14:00:00 done\n",
      "2018-08-05 15:00:00 done\n",
      "2018-08-05 16:00:00 done\n",
      "2018-08-05 17:00:00 done\n",
      "2018-08-05 18:00:00 done\n",
      "2018-08-05 19:00:00 done\n",
      "2018-08-05 20:00:00 done\n",
      "2018-08-05 21:00:00 done\n",
      "2018-08-05 22:00:00 done\n",
      "2018-08-05 23:00:00 done\n",
      "result G: \n",
      "[0.007454425096511841, 0.011010423302650452, 0.014587655663490295, 0.012068361043930054, 0.010283604264259338, 0.008225038647651672, 0.02036626636981964, 0.11143942922353745, 0.6640895009040833, 1.7087124586105347, 2.5800700187683105, 3.390260696411133, 3.8186681270599365, 3.736785888671875, 3.6844940185546875, 3.2367734909057617, 2.482016086578369, 1.4789804220199585, 0.48836737871170044, 0.09540407359600067, 0.014725789427757263, 0.007164940237998962, 0.007570639252662659, 0.009823217988014221]\n",
      "====================\n",
      "result C: \n",
      "[2.0349221229553223, 2.310868263244629, 2.527947425842285, 1.9704489707946777, 2.707611083984375, 3.127387285232544, 2.142369270324707, 2.0020384788513184, 1.4365193843841553, 2.081331253051758, 2.500291109085083, 2.4399163722991943, 2.9848427772521973, 3.5208895206451416, 3.569493293762207, 3.2687344551086426, 4.291411876678467, 3.3484363555908203, 3.5534956455230713, 3.4432592391967773, 3.2256476879119873, 3.3571996688842773, 2.5035321712493896, 1.675137996673584]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def Predict(data, last_date):\n",
    "    trained_modelG = keras.models.load_model('modeG.h5')\n",
    "    trained_modelC = keras.models.load_model('modeC.h5')\n",
    "    \n",
    "    date = datetime.datetime(2018, last_date[0], last_date[1], last_date[2], 0, 0)\n",
    "    \n",
    "    resultG = []\n",
    "    resultC = []\n",
    "    \n",
    "    SIZE = 24 # 1 day / 24 hours\n",
    "    for _ in range(SIZE):\n",
    "        predictG = trained_modelG.predict(data)[0]\n",
    "        predictC = trained_modelC.predict(data)[0]\n",
    "        \n",
    "        \n",
    "        resultG.append(float(predictG))\n",
    "        resultC.append(float(predictC))\n",
    "        \n",
    "        date += datetime.timedelta(hours=1)\n",
    "        \n",
    "        tmp = [predictG[0], predictC[0], date.month, date.day, date.hour]\n",
    "        \n",
    "        data = np.array([np.vstack((data[0], tmp))[1:]])\n",
    "        \n",
    "        print('{} done'.format(date))\n",
    "        \n",
    "    return resultG, resultC\n",
    "\n",
    "last_month = data_to_predict['month'].iloc[-1]\n",
    "last_day = data_to_predict['day'].iloc[-1]\n",
    "last_hour = data_to_predict['hour'].iloc[-1]\n",
    "\n",
    "resultG, resultC = Predict(np.array([data_to_predict.values]), (last_month, last_day, last_hour))\n",
    "print('result G: ')\n",
    "print(resultG)\n",
    "print('='*20)\n",
    "print('result C: ')\n",
    "print(resultC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb4cdb330c5ea7232880705c0e79ad22649a7c708042624124f8ff95c4dc218f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
